# CCL24-Eval 儿童故事常识推理与寓意理解评测
Commonsense Reasoning and Moral Understanding Evaluation in Children's Stories，CRMU
## 1.任务简介
&emsp;&emsp;儿童故事常识推理与寓意理解评测（Evaluation on Commonsense Reasoning and Moral Understanding in Children's Stories，CRMU）任务旨在从常识推理（Commonsense Reasoning）和寓意理解（Moral Understanding）两个任务多角度评价中文预训练语言模型和大型语言模型的常识推理和故事理解能力。本评测包含以下2个子任务：
- 常识推理（Commonsense Reasoning）：基于给定的故事和常识问题，选择正确的候选答案。
-	寓意匹配（Moral Matching）：基于给定的故事，从多个候选答案中选择最符合故事的寓意。
+ 组织者
  + 谭红叶（山西大学）
  + 李  茹（山西大学）
  + 张  虎（山西大学）
  + 俞  奎（合肥工业大学）
+ 负责人
  + 郭亚鑫（山西大学博士生，202112407002@email.sxu.edu.cn）
+ 联系人
  + 闫国航（山西大学硕士生，yanguohang@qq.com）
## 2.评测数据
### 数据集规模
&emsp;&emsp;本评测使用的数据来源于网页收集的经典寓言故事。常识推理任务的问题和选项由人工标注，涉及到的常识类型包含时间常识、空间常识、生物常识、物理常识以及社会常识。寓意理解任务包问题和选项采用人工标注和自动生成结合的方式。所有任务均为中文，各子任务数据量统计如下：
|数据集划分|验证集|测试集|
| :-----:|:-----:|:-----: |
| 常识推理 | 872 |  872  |
| 寓意匹配|  654  |  654  |
##### 标注数据由json格式给出，包含以下内容：
+ dev_
+ test_
### 数据样例
各任务数据样例如下：
#### 常识推理：
```json
{
"story": "乌鸦口渴得要命，飞到一只大水罐旁，水罐里没有很多水，他想尽了办法，仍喝不到。于是，他就使出全身力气去推，想把罐推倒，倒出水来，而大水罐却推也推不动。这时，乌鸦想起了他曾经使用的办法，用口叼着石子投到水罐里，随着石子的增多，罐里的水也就逐渐地升高了。最后，乌鸦高兴地喝到了水，解了口渴。",
"question": "文中乌鸦还可以将什么东西丢到罐子里来喝到水?",
"options": "A．石狮子<br>B．乒乓球<br>C．树叶<br>D．玻璃珠",
"answer": "D",
}
```
#### 寓意匹配：
```json
{
"story": "乌鸦口渴得要命，飞到一只大水罐旁，水罐里没有很多水，他想尽了办法，仍喝不到。于是，他就使出全身力气去推，想把罐推倒，倒出水来，而大水罐却推也推不动。这时，乌鸦想起了他曾经使用的办法，用口叼着石子投到水罐里，随着石子的增多，罐里的水也就逐渐地升高了。最后，乌鸦高兴地喝到了水，解了口渴。",
"question": "文中乌鸦还可以将什么东西丢到罐子里来喝到水?",
"options": "A．石狮子<br>B．乒乓球<br>C．树叶<br>D．玻璃珠",
"answer": "D",
}
```
## 3.评价标准
各子任务评测指标如下：
|任务|评价指标|解释|
| :---:|:---:|:---: |
| 常识推理 | Accuracy |  答案准确率  |
| 寓意匹配|  Accuracy  |  答案准确率  |


参赛模型的最终评测成绩取上述所有评价指标的平均值。
#### 注意：各位选手在参赛的同时也要认真撰写中英文技术报告，它也是评分的重要依据。同时优秀的中英文评测报告将有机会收录到ACL/CCL Anthology！
## 4.评测赛程
### 报名方式： 

### 赛程安排：
+ 报名时间：204年3月1日-4月9日
+ 开发集发布：2024年4月10日
  + 报名结束后在智源指数平台上获取评测数据
+ 测试集发布：2024年5月13日
+ 最终测试结果提交截至：2024年5月20日
+ 公布测试结果：2024年5月31日
+ 中英文技术报告提交：
+ 中英文技术报告反馈：
+ 中英文评测论文提交：
+ 公布获奖名单：
+ 评测论文录用通知：
+ 论文Camera Ready提交：
+ 评测研讨会及颁奖：2024年
### 评测网址
Github链接：https://github.com/SXU-YaxinGuo/CRMU       
智源指数平台：
### 结果提交：
本次评测结果在智源指数平台上进行提交，
##### 最终结果提交
最终测试结果提交截止日前，参赛者需要提交三个文件:     
1)输出结果文件：该文件是以utf-8为编码格式的json文件，其中的内容格式与验证集保持一致，结果文件格式不正确不予计算成绩。该文件命名为：。     
2)模型文件：评测使用的模型，所提交模型必须真实可复现，文件命名为：model.zip。        
3)模型说明文档：该文档是docx文件，其中的内容为模型代码运行调试流程，文件命名为： 。文件格式示例如下：     
GCRC_advRobust.zip   
&emsp;&emsp; 
&emsp;&emsp;
&emsp;&emsp; 
## 5.奖项设置
开放赛道和封闭赛道都将评选出如下奖项。由中国中文信息学会计算语言学专委会（CIPS-CL）为获奖队伍提供荣誉证书。

|奖项	|一等奖|	二等奖|	三等奖|
|:-----:|:----:|:----:|:----:|
|数量	|1名|	2名	|3名|
|奖励	|荣誉证书	|荣誉证书	|荣誉证书|

##### 注意事项
+ 由于版权保护问题，GCRC_advRobust数据集只免费提供给用户用于非盈利性科学研究使用，参赛人员不得将数据用于任何商业用途。如果用于商业产品，请联系谭红叶老师，联系邮箱tanhongye@sxu.edu.cn。 
+ 每名参赛选手只能参加一支队伍，一旦发现某选手以注册多个账号的方式参加多支队伍，将取消相关队伍的参赛资格。
+ 数据集的具体内容、范围、规模及格式以最终发布的真实数据集为准。针对测试集，参赛人员不允许执行任何人工标注，并禁止利用正、负对抗选项的构造规则修正答案。
+ 参赛队伍可在参赛期间随时上传测试集的预测结果，智源指数平台每天可提交5次，系统会实时更新当前最新榜单排名情况，严禁参赛团队注册其它账号多次提交。
+ 允许使用公开和选手个人/组织内部的代码、工具、外部数据（从其他渠道获得的标注数据）等，但需要保证参赛结果可以复现。
+ 在数据处理、模型训练和预测等任意阶段使用大模型的参赛队伍只能参加开放赛道的评测。
+ 开放赛道的参赛队伍必须协助评测组织者对测试结果进行验证，包含且不限于复现大模型结果、提供prompt设计策略等，否则成绩无效。
+ 算法与系统的知识产权归参赛队伍所有。要求最终结果排名前6的队伍提供算法代码与系统报告（包括方法说明、数据处理、参考文献和使用开源工具、外部数据等信息）。提交完毕将采用随机交叉检查的方法对各个队伍提交的模型进行检验，如果在排行榜上的结果无法复现，将取消获奖资格。
+ 参赛团队需保证提交作品的合规性，若出现下列或其他重大违规的情况，将取消参赛团队的参赛资格和成绩，获奖团队名单依次顺延。重大违规情况如下：
    + 使用小号、串通、剽窃他人代码等涉嫌违规、作弊行为；
    + 团队提交的材料内容不完整，或提交任何虚假信息；
    + 参赛团队无法就作品疑议进行足够信服的解释说明；    

### 评测单位
山西大学 合肥工业大学
